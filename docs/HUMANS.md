# HUMANS

The code in this repo is primarily generated by AI agents. 


# Step 1 -- Blueprint
The first step in the process was to have AIs create proposed architectural documents. I ended up asking GPT-5 Pro Claude, Sonnet 4.5, and Gemini Pro 2.5 on all in deep research mode to propose an architectural document. The final solution that I selected was the following: https://g.co/gemini/share/ef5097947855

The prompt was originally dictated using Wispr in a somewhat rambling style, but no effort was made to clean up the prompt after I dictated it. This is typically the style I use for most prompting. After I dictated the prompt once, I would just copy and paste it to the two other platforms I needed to test. 

# Step 2 -- Devops for God
The second step involved me creating a repo with features that could be convenient for an AI agent to operate in, whether it's working locally or, in the case of Codex or Devin, working in the cloud. The idea was to create a test script that properly invoked `firebase emulator:exec` so that it could create a transient database that would be torn down after the tests were run so there's always a clean state and also to introduce a test harness that also could use a special HTTP request to delete the database. I also set up minimal GitHub workflows and documentation.

# Step 3 -- Let 'er Rip
I created an AGENTS.md file and then gave it a go on multiple platforms. I gave Claude, Codex, and Jules each opportunity to implement Phase I of the roadmap. Actually, Codex had two chances because they have a button that makes it easy to launch multiple attempts. All of them created PRs, which you can look through. Ultimately, I spent the most time with Claude's PR. All of them ended up using lots of mocking in the unit test, so they sort of seem like pointless unit tests. So I had Claude actually use the Firestore emulator with live objects. Then I realized that my original prompt was a little ambiguous, and the way Claude chose to support async was a little convoluted. It basically didn't realize that it should be using the async native Firestore client. So, I had it resolve that. This should be visible in the various commits that are part of that PR. Then I asked to do a phase one analysis to see what's the difference between what was actually created versus what was in the phase one objectives. Finally, I had it tweak the architecture document to be clear about how to implement async and sync code and also to prefer live emulation vs. mocking whenever possible. Then, just kind of flesh out more of the details of what the architecture looks like at this point and what the current status of the project is.   Finally I asked it to create a Jupyter notebook that shows how the functionality in Phase I works. 

Then there was about 2 hours where I read through the code, looked for UX and other kinds of issues, and had Claude do micro-tweaks. Things that would be 10-20 lines of grunt work that I wouldn't want to do, but it's more traditional agent coding. 

In all phase one took about 20 hours of wall time, I think. During that time, Claude probably ran for maybe an hour across 11 sessions. The first couple or maybe 10 minutes, or just a little bit longer. But I really couldn't give it something that took hours to do. Maybe it's because it had a pretty good testing harness so that it could actually run tests without making silly mistakes using the emulator. My time was basically about 15 minutes talking to, creating prompts, I used Wispr Flow to create prompts. I don't really type them, I was speaking them. Then maybe like an hour or so, maybe 30 minutes or so watching Claude as it ran. Maybe a couple hours reading the code. Trying to understand what went wrong, trying to think about what's the best way to solve the problem and then telling Claude kind of what to do. My guess is that this amount of work would have taken like two to three seasoned engineers a couple of weeks to do, with a certain amount of fatigue involved. Certainly, if you include things like documentation, and demos, and docstrings, and so forth, and testing. 

# Phase 2
Phase II ended up being broken up into two parts. I again offered this to both Codex and Claude. Codex had two solutions that looked reasonable, but they also had a full-time chance to investigate them. I couldn't get them published as a PR, I think because of the way I launched them. Anyway, Claude looked fine, so I just went with that. Claude chose to basically do 80% of phase one in the first section, and it looks fine. The whole process probably took 30 minutes, but it basically did everything. I kind of went in and asked it to create a demo notebook. Then, for part two of this phase, it implemented some querying logic. Here, I just asked it to create the logic, get a test, create a demo notebook, and update the STATUS.md to reflect where you are. That just seems to have worked fine. Again, that took about 20 minutes. I didn't look over the shoulder of Claude in this case, there wasn't too much to really grok. It just seemed to work. I think now we're in a place where it's just sort of moving along. So again, given the subtlety of some of these things, I think it would take in several days for an engineering team to do this. And at this point, I would say we are pretty much 50% there. This is mildly useful and on its way to being extremely useful.  

